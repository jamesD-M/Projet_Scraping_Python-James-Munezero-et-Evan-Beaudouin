{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a65c22e",
   "metadata": {},
   "source": [
    "# 📚 Projet – Books to Scrape\n",
    "\n",
    "## ⏱️ Temps Estimé : **300 minutes (5 heures)**\n",
    "\n",
    "Ce projet vous guidera dans la création d’un **web scraper** pour le site [Books to Scrape](https://books.toscrape.com/). Il est conçu comme un exercice pratique pour s’entraîner avec **Python, requests, Pandas et l’analyse de données**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcdfc2",
   "metadata": {},
   "source": [
    "## 🎯 Contexte\n",
    "\n",
    "L’équipe marketing d’une librairie en ligne souhaite mieux comprendre son catalogue. Elle veut collecter des informations sur tous les livres, analyser les catégories, les prix, les notes et la disponibilité en stock.\n",
    "\n",
    "En tant que data scientist, votre mission est de **scraper le site web** et de livrer des jeux de données structurés ainsi que des premiers insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b74b63",
   "metadata": {},
   "source": [
    "## ✅ Objectifs\n",
    "\n",
    "1. Scraper le site [Books to Scrape](https://books.toscrape.com/)\n",
    "2. Extraire pour chaque livre :\n",
    "   - Titre\n",
    "   - Prix\n",
    "   - Disponibilité en stock\n",
    "   - Note\n",
    "   - URL du produit\n",
    "   - URL de l’image\n",
    "   - UPC\n",
    "   - Catégorie\n",
    "3. Gérer la **pagination** sur toutes les pages\n",
    "4. Sauvegarder les résultats dans **un CSV par catégorie**\n",
    "5. Télécharger les images des couvertures de livres dans des dossiers par catégorie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80118e",
   "metadata": {},
   "source": [
    "## 📦 Livrables\n",
    "\n",
    "- Fichiers CSV : `outputs/csv/category_<slug>.csv`\n",
    "- Images : `outputs/images/<category>/<upc>_<slug-title>.jpg`\n",
    "- Optionnel : Un notebook de nettotage et d’exploration qui analyse les prix, notes et stocks avec Pandas et Quelques visualisation à réaliser avec les packages que vous préféré."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964718c",
   "metadata": {},
   "source": [
    "## 🛠 Étapes suggérées\n",
    "\n",
    "1. Commencez par scraper **une page produit** et extraire les champs demandés\n",
    "2. Étendez votre code à **une catégorie** (gestion de plusieurs pages)\n",
    "3. Généralisez votre scraper à **toutes les catégories**\n",
    "4. Sauvegardez les résultats dans des fichiers CSV\n",
    "5. Étendez le scraper pour aussi **télécharger les images**\n",
    "6. (Optionnel) Explorez le dataset avec Pandas (prix moyen par catégorie, distribution des notes, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ffcdb",
   "metadata": {},
   "source": [
    "# 🛠 Étapes détaillées\n",
    "\n",
    "## 🟢 Phase 1 – Construire pas à pas (dans un seul script au début)\n",
    "1. **Récupérer une page** → utiliser `requests` pour télécharger le HTML.  \n",
    "2. **Extraire les titres** → avec `Selector`, récupérer les noms des livres sur la page d’accueil.  \n",
    "3. **Extraire les détails** → ouvrir la page d’un livre et extraire :\n",
    "   - titre  \n",
    "   - prix  \n",
    "   - disponibilité en stock  \n",
    "   - note (rating)  \n",
    "   - UPC  \n",
    "   - **URL de l’image** (utiliser `.css(\"img::attr(src)\")` + `urljoin` pour obtenir le lien absolu)  \n",
    "4. **Gérer une catégorie** → collecter toutes les URLs de livres dans une page de catégorie.  \n",
    "5. **Gérer plusieurs pages** → suivre le bouton `\"li.next a\"` jusqu’à ce qu’il n’y en ait plus.  \n",
    "6. **Sauvegarder les résultats** → écrire les résultats dans un fichier CSV (`outputs/csv/category_<name>.csv`).  \n",
    "7. **Télécharger les images** → utiliser l’URL de l’image pour la télécharger avec `requests` :\n",
    "   - Chemin : `outputs/images/<categorie>/<upc>_<slug-title>.jpg`  \n",
    "8. **Gérer plusieurs catégories** → boucler sur toutes les catégories depuis la page d’accueil.  \n",
    "\n",
    "\n",
    "## 🟡 Phase 2 – Organiser le projet\n",
    "Une fois que le code fonctionne, séparer en plusieurs fichiers :  \n",
    "- `parsers.py` → fonctions de scraping (par ex. `parse_list_page`, `parse_product_page`, `get_category_links`)  \n",
    "- `utils.py` → fonctions utilitaires (par ex. `write_csv`, `download_file`, `ensure_dir`)  \n",
    "- `settings.py` → constantes (`BASE_URL`, `HEADERS`, `DEFAULT_DELAY`, `TIMEOUT`)  \n",
    "- `scrape.py` → script principal (seulement `main()` + argparse), appelle les fonctions des autres fichiers  \n",
    "\n",
    "\n",
    "## 🔵 Phase 3 – Automatiser avec la ligne de commande (CLI)\n",
    "Ajouter des **options argparse** dans `scrape.py` :  \n",
    "- `--categories Travel,Poetry` → scraper seulement certaines catégories  \n",
    "- `--max-pages 1` → limiter le scraping pour des tests rapides  \n",
    "- `--delay 1` → ajouter un délai entre les requêtes  \n",
    "- `--outdir outputs` → changer le dossier de sortie  \n",
    "\n",
    "Exemple d’utilisation :\n",
    "```bash\n",
    "python scrape.py --categories Travel --max-pages 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937efb57",
   "metadata": {},
   "source": [
    "## 🟣 Phase 4 – Documenter\n",
    "- Créer un fichier `README.md` avec :\n",
    "  - L’objectif du projet et son contexte  \n",
    "  - Comment installer les dépendances (`pip install -r requirements.txt`)  \n",
    "  - Comment exécuter le scraper  \n",
    "  - Quelques exemples de commandes  \n",
    "\n",
    "\n",
    "## 🔴 Phase 5 – Partager\n",
    "- Publier votre projet sur GitHub avec :\n",
    "  - Les fichiers de code (`scrape.py`, `parsers.py`, `utils.py`, `settings.py`)  \n",
    "  - Un exemple de `README.md`  \n",
    "  - Un dossier `outputs/` vide avec un fichier `.gitkeep` pour conserver la structure  \n",
    "\n",
    "\n",
    "## 🟠 Phase 6 – Explorer les données\n",
    "Ouvrir un **notebook Jupyter** pour analyser les données extraites :\n",
    "\n",
    "1. **Charger un CSV** avec Pandas :\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   df = pd.read_csv(\"outputs/csv/category_travel.csv\")\n",
    "   df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ba8f6",
   "metadata": {},
   "source": [
    "## 📝 Critères d’évaluation\n",
    "\n",
    "- 💡 Effort et compréhension :  \n",
    "  - Je valorise **vos propres essais** plus qu’un simple copier-coller depuis ChatGPT ou Internet.  \n",
    "  - Même un progrès partiel, des commentaires clairs dans le code, ou différentes tentatives montrent un véritable apprentissage.  \n",
    "  - Vous devez être capable d’**expliquer votre code** lors de la relecture ou de la présentation.  \n",
    "\n",
    "- 🌟 Points bonus pour la partie analyse de la donnée à la fin avec un notebook et de la recherche sur itnernet de comment faire des visualisations pertinentes et créative dans le notebook (vous pouvez explorer la librarie **plotly express**).  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
