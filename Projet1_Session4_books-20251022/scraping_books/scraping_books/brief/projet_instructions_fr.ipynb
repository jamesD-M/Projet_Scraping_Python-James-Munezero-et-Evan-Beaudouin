{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a65c22e",
   "metadata": {},
   "source": [
    "# ğŸ“š Projet â€“ Books to Scrape\n",
    "\n",
    "## â±ï¸ Temps EstimÃ© : **300 minutes (5 heures)**\n",
    "\n",
    "Ce projet vous guidera dans la crÃ©ation dâ€™un **web scraper** pour le site [Books to Scrape](https://books.toscrape.com/). Il est conÃ§u comme un exercice pratique pour sâ€™entraÃ®ner avec **Python, requests, Pandas et lâ€™analyse de donnÃ©es**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcdfc2",
   "metadata": {},
   "source": [
    "## ğŸ¯ Contexte\n",
    "\n",
    "Lâ€™Ã©quipe marketing dâ€™une librairie en ligne souhaite mieux comprendre son catalogue. Elle veut collecter des informations sur tous les livres, analyser les catÃ©gories, les prix, les notes et la disponibilitÃ© en stock.\n",
    "\n",
    "En tant que data scientist, votre mission est de **scraper le site web** et de livrer des jeux de donnÃ©es structurÃ©s ainsi que des premiers insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b74b63",
   "metadata": {},
   "source": [
    "## âœ… Objectifs\n",
    "\n",
    "1. Scraper le site [Books to Scrape](https://books.toscrape.com/)\n",
    "2. Extraire pour chaque livre :\n",
    "   - Titre\n",
    "   - Prix\n",
    "   - DisponibilitÃ© en stock\n",
    "   - Note\n",
    "   - URL du produit\n",
    "   - URL de lâ€™image\n",
    "   - UPC\n",
    "   - CatÃ©gorie\n",
    "3. GÃ©rer la **pagination** sur toutes les pages\n",
    "4. Sauvegarder les rÃ©sultats dans **un CSV par catÃ©gorie**\n",
    "5. TÃ©lÃ©charger les images des couvertures de livres dans des dossiers par catÃ©gorie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df80118e",
   "metadata": {},
   "source": [
    "## ğŸ“¦ Livrables\n",
    "\n",
    "- Fichiers CSV : `outputs/csv/category_<slug>.csv`\n",
    "- Images : `outputs/images/<category>/<upc>_<slug-title>.jpg`\n",
    "- Optionnel : Un notebook de nettotage et dâ€™exploration qui analyse les prix, notes et stocks avec Pandas et Quelques visualisation Ã  rÃ©aliser avec les packages que vous prÃ©fÃ©rÃ©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0964718c",
   "metadata": {},
   "source": [
    "## ğŸ›  Ã‰tapes suggÃ©rÃ©es\n",
    "\n",
    "1. Commencez par scraper **une page produit** et extraire les champs demandÃ©s\n",
    "2. Ã‰tendez votre code Ã  **une catÃ©gorie** (gestion de plusieurs pages)\n",
    "3. GÃ©nÃ©ralisez votre scraper Ã  **toutes les catÃ©gories**\n",
    "4. Sauvegardez les rÃ©sultats dans des fichiers CSV\n",
    "5. Ã‰tendez le scraper pour aussi **tÃ©lÃ©charger les images**\n",
    "6. (Optionnel) Explorez le dataset avec Pandas (prix moyen par catÃ©gorie, distribution des notes, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ffcdb",
   "metadata": {},
   "source": [
    "# ğŸ›  Ã‰tapes dÃ©taillÃ©es\n",
    "\n",
    "## ğŸŸ¢ Phase 1 â€“ Construire pas Ã  pas (dans un seul script au dÃ©but)\n",
    "1. **RÃ©cupÃ©rer une page** â†’ utiliser `requests` pour tÃ©lÃ©charger le HTML.  \n",
    "2. **Extraire les titres** â†’ avec `Selector`, rÃ©cupÃ©rer les noms des livres sur la page dâ€™accueil.  \n",
    "3. **Extraire les dÃ©tails** â†’ ouvrir la page dâ€™un livre et extraire :\n",
    "   - titre  \n",
    "   - prix  \n",
    "   - disponibilitÃ© en stock  \n",
    "   - note (rating)  \n",
    "   - UPC  \n",
    "   - **URL de lâ€™image** (utiliser `.css(\"img::attr(src)\")` + `urljoin` pour obtenir le lien absolu)  \n",
    "4. **GÃ©rer une catÃ©gorie** â†’ collecter toutes les URLs de livres dans une page de catÃ©gorie.  \n",
    "5. **GÃ©rer plusieurs pages** â†’ suivre le bouton `\"li.next a\"` jusquâ€™Ã  ce quâ€™il nâ€™y en ait plus.  \n",
    "6. **Sauvegarder les rÃ©sultats** â†’ Ã©crire les rÃ©sultats dans un fichier CSV (`outputs/csv/category_<name>.csv`).  \n",
    "7. **TÃ©lÃ©charger les images** â†’ utiliser lâ€™URL de lâ€™image pour la tÃ©lÃ©charger avec `requests` :\n",
    "   - Chemin : `outputs/images/<categorie>/<upc>_<slug-title>.jpg`  \n",
    "8. **GÃ©rer plusieurs catÃ©gories** â†’ boucler sur toutes les catÃ©gories depuis la page dâ€™accueil.  \n",
    "\n",
    "\n",
    "## ğŸŸ¡ Phase 2 â€“ Organiser le projet\n",
    "Une fois que le code fonctionne, sÃ©parer en plusieurs fichiers :  \n",
    "- `parsers.py` â†’ fonctions de scraping (par ex. `parse_list_page`, `parse_product_page`, `get_category_links`)  \n",
    "- `utils.py` â†’ fonctions utilitaires (par ex. `write_csv`, `download_file`, `ensure_dir`)  \n",
    "- `settings.py` â†’ constantes (`BASE_URL`, `HEADERS`, `DEFAULT_DELAY`, `TIMEOUT`)  \n",
    "- `scrape.py` â†’ script principal (seulement `main()` + argparse), appelle les fonctions des autres fichiers  \n",
    "\n",
    "\n",
    "## ğŸ”µ Phase 3 â€“ Automatiser avec la ligne de commande (CLI)\n",
    "Ajouter des **options argparse** dans `scrape.py` :  \n",
    "- `--categories Travel,Poetry` â†’ scraper seulement certaines catÃ©gories  \n",
    "- `--max-pages 1` â†’ limiter le scraping pour des tests rapides  \n",
    "- `--delay 1` â†’ ajouter un dÃ©lai entre les requÃªtes  \n",
    "- `--outdir outputs` â†’ changer le dossier de sortie  \n",
    "\n",
    "Exemple dâ€™utilisation :\n",
    "```bash\n",
    "python scrape.py --categories Travel --max-pages 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937efb57",
   "metadata": {},
   "source": [
    "## ğŸŸ£ Phase 4 â€“ Documenter\n",
    "- CrÃ©er un fichier `README.md` avec :\n",
    "  - Lâ€™objectif du projet et son contexte  \n",
    "  - Comment installer les dÃ©pendances (`pip install -r requirements.txt`)  \n",
    "  - Comment exÃ©cuter le scraper  \n",
    "  - Quelques exemples de commandes  \n",
    "\n",
    "\n",
    "## ğŸ”´ Phase 5 â€“ Partager\n",
    "- Publier votre projet sur GitHub avec :\n",
    "  - Les fichiers de code (`scrape.py`, `parsers.py`, `utils.py`, `settings.py`)  \n",
    "  - Un exemple de `README.md`  \n",
    "  - Un dossier `outputs/` vide avec un fichier `.gitkeep` pour conserver la structure  \n",
    "\n",
    "\n",
    "## ğŸŸ  Phase 6 â€“ Explorer les donnÃ©es\n",
    "Ouvrir un **notebook Jupyter** pour analyser les donnÃ©es extraites :\n",
    "\n",
    "1. **Charger un CSV** avec Pandas :\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   df = pd.read_csv(\"outputs/csv/category_travel.csv\")\n",
    "   df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ba8f6",
   "metadata": {},
   "source": [
    "## ğŸ“ CritÃ¨res dâ€™Ã©valuation\n",
    "\n",
    "- ğŸ’¡ Effort et comprÃ©hension :  \n",
    "  - Je valorise **vos propres essais** plus quâ€™un simple copier-coller depuis ChatGPT ou Internet.  \n",
    "  - MÃªme un progrÃ¨s partiel, des commentaires clairs dans le code, ou diffÃ©rentes tentatives montrent un vÃ©ritable apprentissage.  \n",
    "  - Vous devez Ãªtre capable dâ€™**expliquer votre code** lors de la relecture ou de la prÃ©sentation.  \n",
    "\n",
    "- ğŸŒŸ Points bonus pour la partie analyse de la donnÃ©e Ã  la fin avec un notebook et de la recherche sur itnernet de comment faire des visualisations pertinentes et crÃ©ative dans le notebook (vous pouvez explorer la librarie **plotly express**).  \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
